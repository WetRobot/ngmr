% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NgramModel.R
\name{NgramModel}
\alias{NgramModel}
\title{Ngram Model}
\usage{
NgramModel(n = 3L, alpha = 1, unseen_alpha = 1, normalise_length = TRUE)
}
\arguments{
\item{n}{\verb{[integer]} (default \code{3L})

The "n" in "ngram". Length of substring to use.
E.g. "<\if{html}{\out{<hello>}}" has substrings \code{c("<<h", "<he", "hel", "ell", "llo", "lo>")}
with \code{n=3}.}

\item{alpha}{\verb{[numeric]} (default \code{1.0})

Dirichlet model alpha parameter. Higher means all transition probabilities
approach \code{1/n} where \code{n} is the number of possible states.}

\item{unseen_alpha}{\verb{[numeric]} (default \code{1.0})

Similar to alpha, but for unseen data. E.g. if substring "<<x" has never
been seen in the training data, it would have a probability of zero
with \code{unseen_alpha = 0.0} but a probability above zero with
\code{unseen_alpha > 0.0}.}

\item{normalise_length}{\verb{[logical]} (default \code{TRUE})

The Markov model has an inherent length bias, where long strings become very
unlikely to observed. With \code{normalise_length = TRUE}, slot \code{lpmf}
returns the geometric mean of the ngrams of a string instead of the raw
probability (at the log scale). Note that these are no longer probabilities
--- if evaluated on all possible strings, the sum would be larger than one.}
}
\description{
Fit and evaluate a character ngram Markov model.
}
\section{Slots}{

\describe{
\item{\code{update}}{Update Markov model with additional texts.
It has parameter \code{texts} \verb{[character]}.}

\item{\code{lpmf}}{Evaluate Markov model log-probabilities.
It has parameters \code{texts} \verb{[character]} (no default) and
\code{n_threads} \verb{[integer]} (default \code{1L}).
The latter controls the number of threads to use when evaluating
log-probabilities. Handy when \code{texts} is large.}
}}

\examples{
# ngmr::NgramModel
nm <- ngmr::NgramModel()
nm@update("<<a>")
nm@lpmf("<<b>")
}
